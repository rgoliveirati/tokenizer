# app_eda_pdfs_ngram_metrica.py
# ---------------------------------------------------------------
# EDA de PDFs: WordPiece¬†√ó¬†Reconstru√≠dos, m√©tricas completas
# (com/sem stopwords), n‚Äëgrams, PoS, LDA, dendrograma
# e resumo via Groq ‚Äî¬†com explica√ß√µes em cada gr√°fico.
# ---------------------------------------------------------------

# === IMPORTS ====================================================
import re, string, json, requests
import pandas as pd, numpy as np
import matplotlib.pyplot as plt; import seaborn as sns
from collections import Counter
from wordcloud import WordCloud
import streamlit as st
import PyPDF2, nltk, spacy
from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize, word_tokenize
from langdetect import detect
from transformers import BertTokenizerFast
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from scipy.cluster.hierarchy import linkage, dendrogram
from scipy import stats

# --- baixar recursos NLTK (primeira execu√ß√£o) -------------------
nltk.download("punkt")
nltk.download("stopwords")
nltk.download("averaged_perceptron_tagger")
try: nltk.data.find("taggers/averaged_perceptron_tagger_eng")
except LookupError: nltk.download("averaged_perceptron_tagger_eng")
try: nltk.data.find("tokenizers/punkt_tab")
except LookupError: nltk.download("punkt_tab")

# === CONFIG =====================================================
MODEL_NAME = "bert-base-uncased"
N_TOPICS   = 5
TOP_N      = 20

# --- spaCy PT ---------------------------------------------------
try:    nlp_pt = spacy.load("pt_core_news_sm")
except: nlp_pt = None

# ================================================================
# ------------------ FUN√á√ïES B√ÅSICAS -----------------------------
def extrair_texto_pdf(file):
    return " ".join(p.extract_text() for p in PyPDF2.PdfReader(file).pages)

def detectar_idioma(txt):
    try: return detect(txt)
    except: return "en"

def limpar_texto(t):
    t = t.lower()
    t = re.sub(r"\s+", " ", t)
    return re.sub(f"[{re.escape(string.punctuation)}]", "", t).strip()

# ------------------ SENTEN√áAS & PoS -----------------------------
def sentencas(txt, lang):
    if lang.startswith("pt") and nlp_pt:
        return [s.text.strip() for s in nlp_pt(txt).sents]
    return sent_tokenize(txt)

def tokens_palavras(sent, lang):
    if lang.startswith("pt") and nlp_pt:
        return [t.text for t in nlp_pt(sent)]
    return word_tokenize(sent)

def pos_counts(tokens, lang):
    nouns=verbs=preps=0
    if lang.startswith("pt") and nlp_pt:
        for t in nlp_pt(" ".join(tokens)):
            if t.pos_=="NOUN": nouns+=1
            if t.pos_=="VERB": verbs+=1
            if t.pos_=="ADP":  preps+=1
    else:
        for _,tg in nltk.pos_tag(tokens):
            if tg.startswith("NN"): nouns+=1
            if tg.startswith("VB"): verbs+=1
            if tg=="IN":           preps+=1
    return nouns,verbs,preps

# ------------------ M√âTRICAS TEXTUAIS ---------------------------
def gerar_metricas(txt_raw, tokens, lang, label):
    """Retorna dicion√°rio de m√©tricas para DataFrame final."""
    sents      = sentencas(txt_raw, lang)
    num_sent   = len(sents)
    mean_sent  = np.mean([len(tokens_palavras(s,lang)) for s in sents]) if num_sent else 0
    num_tok    = len(tokens)
    mean_tok   = num_tok/num_sent if num_sent else 0
    freq       = Counter(tokens)
    top10      = ", ".join([w for w,_ in freq.most_common(10)])
    low10      = ", ".join([w for w,_ in freq.most_common()[-10:]]) if len(freq)>=10 else ", ".join(freq)
    n,v,p      = pos_counts(tokens, lang)
    return dict(
        Conjunto                = label,
        N¬∫_Senten√ßas            = num_sent,
        M√©dia_Tam_Senten√ßa      = round(mean_sent,2),
        N¬∫_Tokens               = num_tok,
        M√©dia_Tokens_por_Sent   = round(mean_tok,2),
        Top10_Tokens            = top10,
        Down10_Tokens           = low10,
        Substantivos            = n,
        Verbos                  = v,
        Preposi√ß√µes             = p
    )

# ------------------ WordPiece reconstrutor ----------------------
def tokens_reconstruidos(txt, tokenizer):
    enc  = tokenizer(txt, return_offsets_mapping=True, add_special_tokens=True)
    toks = tokenizer.convert_ids_to_tokens(enc["input_ids"])
    offs = enc["offset_mapping"]
    out, cur = [], ""
    for t,(s,e) in zip(toks,offs):
        if t in ("[CLS]","[SEP]"): continue
        if t.startswith("##"): cur += t[2:]
        else:
            if cur: out.append(cur)
            cur = t
    if cur: out.append(cur)
    return out

# ------------------ n‚Äëgrams & t√≥picos ---------------------------
def ngram_freq(toks,n): return Counter(" ".join(toks[i:i+n]) for i in range(len(toks)-n+1))

def topicos_globais(textos, n_topics):
    vec = CountVectorizer(stop_words="english"); X = vec.fit_transform(textos)
    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42).fit(X)
    words = vec.get_feature_names_out()
    return [[words[i] for i in comp.argsort()[-10:][::-1]] for comp in lda.components_]

# ------------------ VISUAL HELPERS ------------------------------
def barra(freq,title,cap):
    fig,ax=plt.subplots(figsize=(10,6))
    pares=sorted(freq.items(), key=lambda x:x[1], reverse=True)[:TOP_N]
    sns.barplot(x=[p[1] for p in pares], y=[p[0] for p in pares], ax=ax)
    ax.set_title(title); ax.set_xlabel("Frequ√™ncia"); st.pyplot(fig); st.caption(cap)

def nuvem(freq,title,cap):
    wc=WordCloud(width=1600,height=800,background_color="white").generate_from_frequencies(freq)
    fig,ax=plt.subplots(figsize=(10,6)); ax.imshow(wc); ax.axis("off"); ax.set_title(title)
    st.pyplot(fig); st.caption(cap)

def histo(vals,title,cap):
    fig,ax=plt.subplots(figsize=(10,6))
    sns.histplot(vals,bins=20,kde=True,ax=ax)
    ax.set_title(title); ax.set_xlabel("Tamanho"); st.pyplot(fig); st.caption(cap)

def dendro(texts,top_n=50):
    vec=TfidfVectorizer(stop_words="english",max_features=top_n)
    X=vec.fit_transform(texts).toarray().T; words=vec.get_feature_names_out()
    fig,ax=plt.subplots(figsize=(16,8))
    dendrogram(linkage(X,method="ward"),labels=words,leaf_rotation=90,leaf_font_size=11,ax=ax)
    ax.set_title("Dendrograma de Palavras‚ÄëChave"); st.pyplot(fig)
    st.caption("Linhas horizontais = uni√£o de clusters; altura = dissimilaridade.")

# ------------------ Resumo Groq ---------------------------------
def resumo_groq(txt):
    try: key = st.secrets["GROQ_API_KEY"]
    except KeyError:
        st.warning("‚ö†Ô∏è Defina GROQ_API_KEY em secrets."); return ""
    r=requests.post("https://api.groq.com/openai/v1/chat/completions",
        headers={"Authorization":f"Bearer {key}","Content-Type":"application/json"},
        data=json.dumps({"model":"meta-llama/llama-4-scout-17b-16e-instruct",
                         "messages":[{"role":"user","content":f"Resuma:\n\n{txt}"}],
                         "temperature":0.3,"max_tokens":1024}))
    return r.json()["choices"][0]["message"]["content"] if r.ok else "Erro Groq"

# === STREAMLIT APP ==============================================
st.set_page_config(page_title="EDA PDFs + m√©tricas", layout="wide")
st.title("üìÑ EDA de PDFs¬†‚Äî WordPiece¬†√ó¬†Reconstru√≠dos, m√©tricas & Groq")

upl = st.sidebar.file_uploader("üìÇ Upload PDFs", type="pdf", accept_multiple_files=True)
if not upl: st.stop()

names=[u.name for u in upl]
pdf_sel=st.sidebar.selectbox("Escolha o PDF:", names)
idx=names.index(pdf_sel); texto_raw=extrair_texto_pdf(upl[idx])

lang = detectar_idioma(texto_raw)
stop = set(stopwords.words("portuguese" if lang.startswith("pt") else "english"))
tok  = BertTokenizerFast.from_pretrained(MODEL_NAME)

texto        = limpar_texto(texto_raw)
tokens_wp    = tok.tokenize(texto)
tokens_wp_ns = [t for t in tokens_wp if t not in stop]
tokens_rec   = tokens_reconstruidos(texto, tok)
tokens_rec_ns= [t for t in tokens_rec if t not in stop]

# ------------- M√âTRICAS COMPLETAS (4 conjuntos) -----------------
df_metrics = pd.DataFrame([
    gerar_metricas(texto_raw, tokens_wp,    lang, "WordPiece¬†+¬†stopwords"),
    gerar_metricas(texto_raw, tokens_wp_ns, lang, "WordPiece¬†‚Äì¬†stopwords"),
    gerar_metricas(texto_raw, tokens_rec,   lang, "Reconstr.¬†+¬†stopwords"),
    gerar_metricas(texto_raw, tokens_rec_ns,lang, "Reconstr.¬†‚Äì¬†stopwords"),
])

# Sidebar¬†‚Äë vis√£o r√°pida (WordPiece com stopwords)
quick = df_metrics.iloc[0]
st.sidebar.markdown("### üìè¬†M√©tricas r√°pidas")
for k in ["N¬∫_Senten√ßas","N¬∫_Tokens","M√©dia_Tokens_por_Sent"]:
    st.sidebar.write(f"**{k.replace('_',' ')}:** {quick[k]}")

# Cabe√ßalho
st.header(f"üìö PDF: `{pdf_sel}` ‚Äî Idioma: **{lang.upper()}**")
c1,c2,c3,c4=st.columns(4)
c1.metric("WP‚Äëtokens",          len(tokens_wp))
c2.metric("WP s/stop",          len(tokens_wp_ns))
c3.metric("Rec‚Äëtokens",         len(tokens_rec))
c4.metric("Rec s/stop",         len(tokens_rec_ns))

# Resumo Groq
st.subheader("üìú Resumo (Groq)")
with st.spinner("Gerando resumo‚Ä¶"):
    st.write(resumo_groq(texto))

st.divider()

# ------------------ FREQU√äNCIAS & N‚ÄëGRAMS -----------------------
def blocos(tokens,label):
    freq_all=Counter(tokens)
    freq_fil=Counter([t for t in tokens if t not in stop])
    barra(freq_all,f"Top {label} (com stopwords)","Barra = contagem.")
    barra(freq_fil,f"Top {label} (sem stopwords)","Stopwords removidas.")
    nuvem(freq_all,f"Nuvem {label} (com stopwords)","Palavras maiores = mais frequentes.")
    nuvem(freq_fil,f"Nuvem {label} (sem stopwords)","Real√ßa termos‚Äëchave.")
    for n in (2,3):
        ng=ngram_freq(tokens,n)
        if ng:
            barra(ng,f"Top {n}-grams ({label})",f"{n}-grams mais frequentes.")
            nuvem(ng,f"Nuvem {n}-grams ({label})","")

st.subheader("üìà WordPiece");       blocos(tokens_wp,"WordPiece")
st.subheader("üìà Reconstru√≠dos");   blocos(tokens_rec,"Reconstru√≠dos")

# ------------------ DISTRIBUI√á√ïES -------------------------------
st.subheader("üîµ Distribui√ß√µes de Tamanho")
histo([len(t) for t in tokens_wp],"WordPiece","Tamanho em caracteres.")
histo([len(t) for t in tokens_wp_ns],"WordPiece (sem stopwords)","")
histo([len(t) for t in tokens_rec],"Reconstru√≠dos","Tokens completos tendem a ser maiores.")
histo([len(t) for t in tokens_rec_ns],"Reconstru√≠dos (sem stopwords)","")

# ------------------ TABELA DE M√âTRICAS --------------------------
st.subheader("üìä M√©tricas completas (4 conjuntos)")
st.dataframe(df_metrics, use_container_width=True)

st.divider()

# ------------------ T√ìPICOS & DENDROGRAMA -----------------------
st.subheader("üß† T√≥picos (LDA global)")
all_clean=[limpar_texto(extrair_texto_pdf(f)) for f in upl]
for i,top in enumerate(topicos_globais(all_clean,N_TOPICS),1):
    st.write(f"**T√≥pico¬†{i}:** {', '.join(top)}")

st.subheader("üå≥ Dendrograma global")
dendro(all_clean)
