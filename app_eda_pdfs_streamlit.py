# app_eda_pdfs_ngram_metrica.py
# ---------------------------------------------------------------
# EDA completa: WordPiece, reconstru√ß√£o, n‚Äëgrams, m√©tricas,
# senten√ßas, PoS, LDA, dendrograma e resumo Groq.
# ---------------------------------------------------------------

# === IMPORTS ====================================================
import re, string, json, requests, os
import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns
from collections import Counter
from wordcloud import WordCloud
import streamlit as st
import PyPDF2, nltk, spacy
from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize, word_tokenize
from langdetect import detect
from transformers import BertTokenizerFast
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from scipy.cluster.hierarchy import linkage, dendrogram
from scipy import stats
nltk.download("punkt"); nltk.download("averaged_perceptron_tagger"); nltk.download("stopwords")
try:
    nltk.data.find("tokenizers/punkt_tab")
except LookupError:
    nltk.download("punkt_tab")

# === CONFIG =====================================================
MODEL_NAME = "bert-base-uncased"
N_TOPICS   = 5
TOP_N      = 20          # Top barras/n‚Äëgrams
# tenta carregar modelo PT; se n√£o existir, apenas None
try:
    nlp_pt = spacy.load("pt_core_news_sm")
except: 
    nlp_pt = None

# ================================================================
# ------------------ FUN√á√ïES UTILIT√ÅRIAS -------------------------
def extrair_texto_pdf(file):
    txt = ""
    for p in PyPDF2.PdfReader(file).pages:
        txt += p.extract_text() + " "
    return txt

def detectar_idioma(txt):
    try:    return detect(txt)
    except: return "en"

def limpar_texto(t):
    t = t.lower()
    t = re.sub(r"\s+", " ", t)
    return re.sub(f"[{re.escape(string.punctuation)}]", "", t).strip()

# ------------------ M√©tricas de senten√ßas/PoS -------------------
def sentencas(txt, lang):
    if lang.startswith("pt") and nlp_pt:
        return [s.text.strip() for s in nlp_pt(txt).sents]
    return sent_tokenize(txt)

def tokens_palavras(sent, lang):
    if lang.startswith("pt") and nlp_pt:
        return [t.text for t in nlp_pt(sent)]
    return word_tokenize(sent)

def pos_counts(tokens, lang):
    nouns=verbs=preps=0
    if lang.startswith("pt") and nlp_pt:
        for t in nlp_pt(" ".join(tokens)):
            if t.pos_=="NOUN": nouns+=1
            if t.pos_=="VERB": verbs+=1
            if t.pos_=="ADP":  preps+=1
    else:
        for _,tg in nltk.pos_tag(tokens):
            if tg.startswith("NN"): nouns+=1
            if tg.startswith("VB"): verbs+=1
            if tg=="IN":           preps+=1
    return nouns,verbs,preps

def gerar_metricas(txt_cln, tokens, lang):
    sents = sentencas(txt_cln, lang)
    num_sent = len(sents)
    mean_sent = np.mean([len(tokens_palavras(s,lang)) for s in sents]) if num_sent else 0
    num_tok = len(tokens); mean_tok = num_tok/num_sent if num_sent else 0
    freq = Counter(tokens)
    top10  = ", ".join([w for w,_ in freq.most_common(10)])
    low10  = ", ".join([w for w,_ in freq.most_common()[-10:]]) if len(freq)>=10 else ", ".join(freq)
    n,v,p = pos_counts(tokens, lang)
    return dict(N¬∫_Senten√ßas=num_sent, M√©dia_Sent_Len=round(mean_sent,2),
                N¬∫_Tokens=num_tok, M√©dia_Tokens_por_Sent=round(mean_tok,2),
                Top10=top10, Down10=low10,
                Substantivos=n, Verbos=v, Preposi√ß√µes=p)

# ------------------ WordPiece reconstrutor ----------------------
def tokens_reconstruidos(txt, tokenizer):
    enc = tokenizer(txt, return_offsets_mapping=True, add_special_tokens=True)
    toks = tokenizer.convert_ids_to_tokens(enc["input_ids"])
    offs = enc["offset_mapping"]
    cur, out = "", []
    for t,(s,e) in zip(toks,offs):
        if t in ("[CLS]","[SEP]"): continue
        if t.startswith("##"):
            cur += t[2:]
        else:
            if cur: out.append(cur)
            cur = t
    if cur: out.append(cur)
    return out

# ------------------ n‚Äëgrams & t√≥picos ---------------------------
def ngram_freq(toks,n): return Counter([" ".join(toks[i:i+n]) for i in range(len(toks)-n+1)])

def topicos_globais(textos, n_topics):
    vec = CountVectorizer(stop_words="english"); X = vec.fit_transform(textos)
    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42).fit(X)
    words = vec.get_feature_names_out()
    return [[words[i] for i in comp.argsort()[-10:][::-1]] for comp in lda.components_]

# ------------------ Visual helpers ------------------------------
def barra(freq, title, cap):
    fig,ax = plt.subplots(figsize=(10,6))
    pares = sorted(freq.items(), key=lambda x:x[1], reverse=True)[:TOP_N]
    sns.barplot(x=[p[1] for p in pares], y=[p[0] for p in pares], ax=ax)
    ax.set_title(title); ax.set_xlabel("Frequ√™ncia"); st.pyplot(fig); st.caption(cap)

def nuvem(freq, title, cap):
    wc = WordCloud(width=1600,height=800,background_color="white").generate_from_frequencies(freq)
    fig,ax=plt.subplots(figsize=(10,6)); ax.imshow(wc); ax.axis("off"); ax.set_title(title)
    st.pyplot(fig); st.caption(cap)

def histo(values,title,cap):
    fig,ax=plt.subplots(figsize=(10,6))
    sns.histplot(values,bins=20,kde=True,ax=ax); ax.set_title(title); ax.set_xlabel("Tamanho")
    st.pyplot(fig); st.caption(cap)

def dendro(texts,top_n=50):
    vec=TfidfVectorizer(stop_words="english",max_features=top_n); X=vec.fit_transform(texts).toarray().T
    words=vec.get_feature_names_out()
    fig,ax=plt.subplots(figsize=(16,8))
    dendrogram(linkage(X,method="ward"),labels=words,leaf_rotation=90,leaf_font_size=11,ax=ax)
    ax.set_title("Dendrograma de Palavras‚ÄëChave"); st.pyplot(fig)
    st.caption("Linhas horizontais indicam uni√£o de clusters; altura = dissimilaridade.")

# ------------------ Resumo Groq ---------------------------------
def resumo_groq(txt):
    try: key = st.secrets["GROQ_API_KEY"]
    except KeyError:
        st.warning("‚ö†Ô∏è Defina GROQ_API_KEY em secrets."); return ""
    payload={"model":"meta-llama/llama-4-scout-17b-16e-instruct",
             "messages":[{"role":"user","content":f"Resuma:\n\n{txt}"}],
             "temperature":0.3,"max_tokens":1024}
    r=requests.post("https://api.groq.com/openai/v1/chat/completions",
                    headers={"Authorization":f"Bearer {key}","Content-Type":"application/json"},
                    data=json.dumps(payload))
    return r.json()["choices"][0]["message"]["content"] if r.ok else "Erro Groq"

# === STREAMLIT ===================================================
st.set_page_config(page_title="EDA PDFs + m√©tricas", layout="wide")
st.title("üìÑ EDA de PDFs com m√©tricas avan√ßadas (n‚Äëgrams, PoS, Groq)")

upl = st.sidebar.file_uploader("üìÇ Upload PDFs", type="pdf", accept_multiple_files=True)
if not upl: st.stop()

names=[u.name for u in upl]; pdf_sel=st.sidebar.selectbox("Escolha o PDF:",names)
idx=names.index(pdf_sel); texto_raw=extrair_texto_pdf(upl[idx])

lang=detectar_idioma(texto_raw)
stop=set(stopwords.words("portuguese" if lang.startswith("pt") else "english"))
tok=BertTokenizerFast.from_pretrained(MODEL_NAME)
texto=limpar_texto(texto_raw)
tokens_wp=tok.tokenize(texto); tokens_rec=tokens_reconstruidos(texto,tok)

# === M√©tricas r√°pidas (sidebar) =================================
metrics=gerar_metricas(texto,tokens_rec,lang)
st.sidebar.markdown("### üìè¬†M√©tricas do PDF")
for k,v in metrics.items(): st.sidebar.write(f"**{k.replace('_',' ')}:** {v}")

# === Cabe√ßalho ===================================================
st.header(f"üìö PDF: `{pdf_sel}` ‚Äî Idioma: **{lang.upper()}**")
c1,c2=st.columns(2); c1.metric("Tokens WordPiece",len(tokens_wp)); c2.metric("Tokens Reconstr.",len(tokens_rec))

# === Resumo ======================================================
st.subheader("üìú Resumo (Groq)")
with st.spinner("Gerando resumo‚Ä¶"): st.write(resumo_groq(texto))

st.divider()

# === Frequ√™ncias, n‚Äëgrams, distribui√ß√µes, estat√≠sticas ===========
def mostra_blocos(tokens,label):
    freq_all=Counter(tokens); freq_flt=Counter([t for t in tokens if t not in stop])
    barra(freq_all,f"Top {label} (com stopwords)","Comprimento da barra = contagem de ocorr√™ncia.")
    barra(freq_flt,f"Top {label} (sem stopwords)","Stopwords removidas.")
    nuvem(freq_all,f"Nuvem {label} (com stopwords)","Palavras maiores = mais frequentes.")
    nuvem(freq_flt,f"Nuvem {label} (sem stopwords)","Real√ßa termos mais relevantes.")
    for n in (2,3):
        ng=ngram_freq(tokens,n)
        if ng: barra(ng,f"Top {n}-grams ({label})",f"Barras = {n}-grams mais frequentes."); nuvem(ng,f"Nuvem {n}-grams ({label})","")

st.subheader("üìà WordPiece"); mostra_blocos(tokens_wp,"WordPiece")
st.subheader("üìà Reconstru√≠dos"); mostra_blocos(tokens_rec,"Reconstru√≠dos")

st.subheader("üîµ Distribui√ß√µes de Tamanho")
histo([len(t) for t in tokens_wp],"WordPiece","Tamanho em caracteres."); 
histo([len(t) for t in tokens_wp if t not in stop],"WordPiece (sem stopwords)",""); 
histo([len(t) for t in tokens_rec],"Reconstru√≠dos","Tokens completos tendem a ser maiores.");
histo([len(t) for t in tokens_rec if t not in stop],"Reconstru√≠dos (sem stopwords)","")

# === Estat√≠sticas detalhadas =====================================
df_stats=pd.DataFrame([
    metrics,   # j√° cont√©m linhas solicitadas
])
st.subheader("üìä Estat√≠sticas detalhadas"); st.dataframe(df_stats,use_container_width=True)

st.divider()

# === T√≥picos & Dendrograma ======================================
st.subheader("üß† T√≥picos (LDA global)")
for i,top in enumerate(topicos_globais([limpar_texto(extrair_texto_pdf(f)) for f in upl],N_TOPICS),1):
    st.write(f"**T√≥pico¬†{i}:** {', '.join(top)}")
st.subheader("üå≥ Dendrograma global"); dendro([limpar_texto(extrair_texto_pdf(f)) for f in upl])
